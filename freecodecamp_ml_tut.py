# -*- coding: utf-8 -*-
"""Freecodecamp -  ML Tut

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fg0rjxWaCyQnWd6ErxbmDa88YBb2hNLC
"""

# Introduction to TensorFlow

import tensorflow as tf

# Creating Tensors

string = tf.Variable ("This is a string", tf.string)
number = tf.Variable (324, tf.int16)
floating = tf.Variable(3.567, tf.float64)


print (string)

#Rank/Degree of Tensor (Number of Dimension in Tensor)

rank1_tensor = tf.Variable(["Test"], tf.string)
tf.rank(rank1_tensor)


rank2_tensor = tf.Variable([["test","ok"],["test","yes"]],tf.string)
tf.rank(rank2_tensor)
#rank2_tensor is a matrix
#rank is given by the  numpy output. this is numpy= 2, therefore rank =2 as well

#Tensor vs Matrix

#All matrices are not tensors, although all Rank 2 tensors are matrices.

# https://www.geeksforgeeks.org/differences-between-a-matrix-and-a-tensor/

#Shape of Tensors

rank2_tensor.shape

#Rank 3 causes an error on purpose
#rank3_tensor = tf.Variable([["test","ok","yes"],["test","yes"]],tf.string)
#rank3_tensor.shape
#The error ( below)
# ValueError: Can't convert non-rectangular Python sequence to Tensor.
# Cant have a different number of elements in one list and another
#number of elements in the second list

#Rank 4 is fixed rank 3

rank4_tensor = tf.Variable([["test","ok","yes"],["test","yes","It doesnt matter how many words there are in the element"]],tf.string)
rank4_tensor.shape
# shape of rank 4:
# TensorShape([2, 3])

#Changing Shape
# tf.reshape() takes 2 args
tensor1 = tf.ones([1,2,3]) # tf.ones() creates a shape [1,2,3] tensor full of ones
# this is 1 interior list, whihc has 2 lists inside of THAT list and each of those lists have 3 elements (i.e. 6 elemets)
tensor2 = tf.reshape(tensor1,[2,3,1]) # 2 lists, 3 inside each of those lists. and each of those lists have 1 elements
tensor3 = tf.reshape(tensor2,[3,-1]) #-1 causes TF to figure out what the final number should be based on the number of elements

print(tensor1)

print(tensor2)

print(tensor3)
#shape=(3, 2) Can multiply these numbers to find out how many elements in total

# Not working, nut not an issue as its not used that much apparently, (according to Tech with Tim)
#with tf.Session as sess:
#  tensor3.eval()
#AttributeError: module 'tensorflow' has no attribute 'Session'

#Apparently Session is no longer a part of tensor flow

# Core Learning Algorithims

# Linear Regression

import numpy as np # can do stuff like matrix add/sub, cross and dot prducts and other stuff
import matplotlib.pyplot as plt #visiulisation of data

x = [1,2,2.5,3,4]
y = [1,4,7,9,15]

plt.plot(x,y,'ro')# plots the values, if there is no 3rd argument default is a line graph (x and y need to have same dimensions)
plt.axis([0,6,0,30]) # [x-starting,x-ending,y-starting,y-ending]

plt.plot(x,y,'ro')
plt.axis([0,6,0,20])

plt.plot(np.unique(x), np.poly1d(np.polyfit(x,y,1))(np.unique(x)))

#pip install -q sklearn
# this is wrong method. Got the method from stackoverflow.
#  https://stackoverflow.com/questions/64467801/sklearn-library-in-google-colab

!pip install scikit-learn

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x   # only neede in notebooks

from __future__ import absolute_import, division, print_function, unicode_literals

import pandas as pd #  allows easy manipulation of datasets, as well visulation of datasets

from IPython.display import clear_output # for this notebook, otherwise not needed
from six.moves import urllib

import tensorflow.compat.v2.feature_column as fc # need a feature coloum when creating algo

# Getting the Data set. From Titanic (from TF website ). Opened in google sheets after downloading it

dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') # training data
dftest = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # testing data

print(dftrain.head())

y_train = dftrain.pop('survived') #removes it from the training data to store it in y_train
y_test = dftest.pop('survived') # same thing as above but fore test data

# Need to seperate the data thats going to be classified from the data thats going to be the input information.
# The 'survivied' is what is being sought after hence its stored seperately

# test data ensures that there is no bias and memoristation i.e. the model(s) are trained properly and giving good results.

#parch = parents and children
#survived is denoted by binary. 0 = dead, 1 = lived
#age is age
#fare = ticket price passenger paid to get onto ship
#deck = what deck they were on( ig its based on ticket price)
#embark town = where they came from?

print(dftrain.head()) # printing this in a seperate block causes it to remove survived from the training df.

print(y_train) #index 0 in dftrain is index 0 in y_train

print(dftrain.loc[0],y_train.loc[10])

#this person who was in 3rd class, was 22, paid 7.25 for his ticket and etc
# did not survive ( "dtype: object 0" )

dftrain["age"]

#dftrain.head() vs print(dftrain.head())
print(dftrain.head())

dftrain.head() # slighlty nicer looking output

dftrain.describe()

dftrain.shape
#627 rows and 9 coloums/attributes(aka features)

y_train.head()
#shows the index on the left coloum and survived in the right coloum

dftrain.age.hist(bins = 20)

dftrain.age.hist(bins = 150) #bins seems to be zoom thing?

dftrain.sex.value_counts().plot(kind ='barh')

dftrain['class'].value_counts().plot(kind ='barh')

pd.concat([dftrain, y_train], axis=1).groupby('sex').survived.mean().plot(kind='barh').set_xlabel('Survived')

# Training and Testing Data

CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',
                       'embark_town', 'alone']
NUMERIC_COLUMNS = ['age', 'fare']

feature_columns = []
for feature_name in CATEGORICAL_COLUMNS:
  vocabulary = dftrain[feature_name].unique()  # gets a list of all unique values from given feature column
  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary)) # linear regression requires creating feature coloums. (look up syntax when needed)
  # this creates a colum as an numpy array that has feature_name and all the vocabulary associated with it

for feature_name in NUMERIC_COLUMNS:
  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))


#incase something is removed.
#WARNING:tensorflow:From <ipython-input-33-fc41aaf068b4>:8: categorical_column_with_vocabulary_list (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
#Instructions for updating:
#Use Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.
#WARNING:tensorflow:From <ipython-input-33-fc41aaf068b4>:12: numeric_column (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
#Instructions for updating:
#Use Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.

print(feature_columns)

dftrain["embark_town"].unique() # the rounded brackets at the end are important

#CREATING THE MODEL

#string = "Creating the Model"
#print(string.upper())

#input function


def make_input_fn(data_df, label_df, num_epochs=100, shuffle=True, batch_size=32): #data_df = the panda df e.g. dftrain or dftest, lable_df = y_train or _test, shuffle = will shuffle the data everytime if True
  def input_function():  # inner function, this will be returned
    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))  # create tf.data.Dataset object with data and its label, ".Dataset.from_tensor_slices " = reduces tensor by slicing along first dimension, why? idk,  dict(data_df)  turns the given df into a dict whihc means it has key: value pairs ig. but it doesnt happen to the lable_df because its the lable?
    if shuffle:
      ds = ds.shuffle(10000)  # randomize order of data, the parameter for shuffle should be bigger than the data set every time
    ds = ds.batch(batch_size).repeat(num_epochs)  # split dataset into batches of 32 and repeat process for number of epochs, splits ds into the number of blocks determined by batch size and then repeat it for the number of epochs?
    return ds  # return a batch of the dataset
  return input_function  # return a function object for use

train_input_fn = make_input_fn(dftrain, y_train)  # here we will call the input_function that was returned to us to get a dataset object we can feed to the model
test_input_fn = make_input_fn(dftest, y_test, num_epochs=1, shuffle=False) # because this is different to the original paramets, when writing the function, the change has to be specified

# Creating the Model

linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)
# We create a linear estimtor by passing the feature columns we created earlier

# WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp98g_c_0c
# the above warning shows up in the vid tut as well

#Training the Model

linear_est.train(train_input_fn)  # train
result = linear_est.evaluate(test_input_fn)  # get model metrics/stats by testing on tetsing data

clear_output()  # clears console output
print(result['accuracy'])  # the result variable is simply a dict of stats about our model

pred_dicts = list(linear_est.predict(test_input_fn))
probs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])

probs.plot(kind='hist', bins=20, title='predicted probabilities')